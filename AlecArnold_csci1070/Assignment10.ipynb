{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is inductive reasoning? Deductive reasoning? Give an example of each, different from the examples given in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inductive reasoning is making assumptions based on past observations.\n",
    "\n",
    "Deductive reasoning is when rules and logic are used to come to a conclusion\n",
    "\n",
    "An example of inductive reasoning is when making a model that can guess if a message is a scam, it uses past resuts from a dataset to make a prediction about any given next message. \n",
    "\n",
    "An example of deductive reasoning is making a program that finds if a number is prime by checking if it is not divisble by certain numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aga20\\AppData\\Local\\Temp\\ipykernel_43704\\4081084899.py:58: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  arr_df = arr_df.interpolate()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>QS_duration</th>\n",
       "      <th>PR_interval</th>\n",
       "      <th>QT_interval</th>\n",
       "      <th>T_interval</th>\n",
       "      <th>P_interval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>63</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>53</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>75</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>71</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>51</td>\n",
       "      <td>84</td>\n",
       "      <td>100</td>\n",
       "      <td>167</td>\n",
       "      <td>321</td>\n",
       "      <td>174</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex  Height  Weight Heart_Rate  QS_duration  PR_interval  QT_interval  \\\n",
       "0   75    0     190      80         63           91          193          371   \n",
       "1   56    1     165      64         53           81          174          401   \n",
       "2   54    0     172      95         75          138          163          386   \n",
       "3   55    0     175      94         71          100          202          380   \n",
       "5   13    0     169      51         84          100          167          321   \n",
       "\n",
       "   T_interval  P_interval  \n",
       "0         174         121  \n",
       "1         149          39  \n",
       "2         185         102  \n",
       "3         179         143  \n",
       "5         174          91  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "\n",
    "column_names = [\"Age\",\"Sex\",\"Height\",\"Weight\", \"QS_duration\", \"PR_interval\",\"QT_interval\",\"T_interval\",\"P_interval\",\"QRS\",\"T\",\"P\",\"QRST\", \"J\",\"Heart_Rate\", \"word1\", \"word2\", \"word3\", \"word4\", \"word5\", \"word6\", \"word7\", \"word8\", \"word9\", \"word10\",\n",
    "    \"word11\", \"word12\", \"word13\", \"word14\", \"word15\", \"word16\", \"word17\", \"word18\", \"word19\", \"word20\",\n",
    "    \"word21\", \"word22\", \"word23\", \"word24\", \"word25\", \"word26\", \"word27\", \"word28\", \"word29\", \"word30\",\n",
    "    \"word31\", \"word32\", \"word33\", \"word34\", \"word35\", \"word36\", \"word37\", \"word38\", \"word39\", \"word40\",\n",
    "    \"word41\", \"word42\", \"word43\", \"word44\", \"word45\", \"word46\", \"word47\", \"word48\", \"word49\", \"word50\",\n",
    "    \"word51\", \"word52\", \"word53\", \"word54\", \"word55\", \"word56\", \"word57\", \"word58\", \"word59\", \"word60\",\n",
    "    \"word61\", \"word62\", \"word63\", \"word64\", \"word65\", \"word66\", \"word67\", \"word68\", \"word69\", \"word70\",\n",
    "    \"word71\", \"word72\", \"word73\", \"word74\", \"word75\", \"word76\", \"word77\", \"word78\", \"word79\", \"word80\",\n",
    "    \"word81\", \"word82\", \"word83\", \"word84\", \"word85\", \"word86\", \"word87\", \"word88\", \"word89\", \"word90\",\n",
    "    \"word91\", \"word92\", \"word93\", \"word94\", \"word95\", \"word96\", \"word97\", \"word98\", \"word99\", \"word100\", \"word101\", \"word102\", \"word103\", \"word104\", \"word105\", \"word106\", \"word107\", \"word108\", \"word109\", \"word110\",\n",
    "    \"word111\", \"word112\", \"word113\", \"word114\", \"word115\", \"word116\", \"word117\", \"word118\", \"word119\", \"word120\",\n",
    "    \"word121\", \"word122\", \"word123\", \"word124\", \"word125\", \"word126\", \"word127\", \"word128\", \"word129\", \"word130\",\n",
    "    \"word131\", \"word132\", \"word133\", \"word134\", \"word135\", \"word136\", \"word137\", \"word138\", \"word139\", \"word140\",\n",
    "    \"word141\", \"word142\", \"word143\", \"word144\", \"word145\", \"word146\", \"word147\", \"word148\", \"word149\", \"word150\",\n",
    "    \"word151\", \"word152\", \"word153\", \"word154\", \"word155\", \"word156\", \"word157\", \"word158\", \"word159\", \"word160\",\n",
    "    \"word161\", \"word162\", \"word163\", \"word164\", \"word165\", \"word166\", \"word167\", \"word168\", \"word169\", \"word170\",\n",
    "    \"word171\", \"word172\", \"word173\", \"word174\", \"word175\", \"word176\", \"word177\", \"word178\", \"word179\", \"word180\",\n",
    "    \"word181\", \"word182\", \"word183\", \"word184\", \"word185\", \"word186\", \"word187\", \"word188\", \"word189\", \"word190\",\n",
    "    \"word191\", \"word192\", \"word193\", \"word194\", \"word195\", \"word196\", \"word197\", \"word198\", \"word199\", \"word200\",    \"word201\", \"word202\", \"word203\", \"word204\", \"word205\", \"word206\", \"word207\", \"word208\", \"word209\", \"word210\",\n",
    "    \"word211\", \"word212\", \"word213\", \"word214\", \"word215\", \"word216\", \"word217\", \"word218\", \"word219\", \"word220\",\n",
    "    \"word221\", \"word222\", \"word223\", \"word224\", \"word225\", \"word226\", \"word227\", \"word228\", \"word229\", \"word230\",\n",
    "    \"word231\", \"word232\", \"word233\", \"word234\", \"word235\", \"word236\", \"word237\", \"word238\", \"word239\", \"word240\",\n",
    "    \"word241\", \"word242\", \"word243\", \"word244\", \"word245\", \"word246\", \"word247\", \"word248\", \"word249\", \"word250\",\n",
    "    \"word251\", \"word252\", \"word253\", \"word254\", \"word255\", \"word256\", \"word257\", \"word258\", \"word259\", \"word260\",\n",
    "    \"word261\", \"word262\", \"word263\", \"word264\", \"word265\"\n",
    "]\n",
    "\n",
    "\n",
    "arr_df = pd.read_csv(\"arrhythmia.data\", names=column_names,sep=\",\")\n",
    "\n",
    "\n",
    "\n",
    "# Eliminate unncessary columns\n",
    "columns = ['Age','Sex','Height','Weight','Heart_Rate','QS_duration','PR_interval','QT_interval','T_interval','P_interval']\n",
    "\n",
    "arr_df = arr_df[columns]\n",
    "\n",
    "# Sex: 0 = male, 1 = female\n",
    "\n",
    "  \n",
    "# Deal with Null values\n",
    "\n",
    "char = '?'\n",
    "\n",
    "arr_df.replace(char, pd.NA, inplace=True) # Get rid of the random question marks\n",
    "arr_df = arr_df.dropna()\n",
    "\n",
    "arr_df = arr_df.interpolate()\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "arr_df.head()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81        61\n",
      "           1       0.88      0.77      0.82        75\n",
      "\n",
      "    accuracy                           0.82       136\n",
      "   macro avg       0.82      0.82      0.82       136\n",
      "weighted avg       0.82      0.82      0.82       136\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aga20\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "1000 fits failed out of a total of 10000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aga20\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aga20\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\aga20\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\aga20\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aga20\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.81904762 0.81904762 ... 0.79365079 0.79365079 0.79365079]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a decision tree model tuned to the best of your abilities. Explain how you tuned it.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "X = arr_df.drop('Sex', axis=1)\n",
    "y = arr_df['Sex']\n",
    "\n",
    "#split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.3, random_state=32)\n",
    "\n",
    "model = tree.DecisionTreeClassifier(max_depth = 7, random_state=32)\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tuning\n",
    "\n",
    "# Grid search to find best parameters for max depth, min samples split, min samples leaf,\n",
    "# and criteria\n",
    "\n",
    "grid = {'max_depth': [1,2,3,4,5,6,7,8,9,10], # Set up grid\n",
    "        'min_samples_split': [1,2,3,4,5,6,7,8,9,10],\n",
    "        'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10],\n",
    "        'criterion': ['gini','entropy']}\n",
    "\n",
    "search = GridSearchCV(estimator=tree.DecisionTreeClassifier(random_state=32),param_grid=grid) # Grid Search\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "parameters = search.best_estimator_ # getting the best parameters\n",
    "\n",
    "y_pred = parameters.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Recall before tuning: 81\n",
    "# Recall after tuning: .82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tuned the decision tree model by creating a grid search that went through different values for the hyper parameters and found the best ones, then put them into the ML model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best n estimators: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7867647058823529"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random forest model tuned to the best of your abilities. Explain how you tuned it.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = arr_df.drop('Sex', axis = 1)\n",
    "y = arr_df['Sex']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, \n",
    "                                                    random_state=32,\n",
    "                                                    stratify=y)\n",
    "\n",
    "# standardize\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "# Random forest\n",
    "rf = RandomForestClassifier(random_state=32)\n",
    "# Tune n_estimators with grid search\n",
    "\n",
    "grid = {'n_estimators': [50, 100, 150, 200, 250]}\n",
    "\n",
    "search = GridSearchCV(estimator = rf, param_grid=grid)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "\n",
    "optimal_n_estimators = search.best_params_['n_estimators']\n",
    "print('best n estimators:',optimal_n_estimators)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = optimal_n_estimators, random_state=32)\n",
    "\n",
    "rf = rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tuned the random forest model by creating a grid search that goes through different \n",
    "\n",
    "values for n_estimators and finds the best value for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an xgboost model tuned to the best of your abilities. Explain how you tuned it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best max depth: 8\n",
      "best learning rate: 0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7426470588235294"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "X = arr_df.drop('Sex', axis = 1)\n",
    "y = arr_df['Sex']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, \n",
    "                                                    random_state=32,\n",
    "                                                    stratify=y)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "grid = {'max_depth': [3,4,5,6,7,8,9,10],\n",
    "        'learning_rate': [.2,.3,.4]}\n",
    "xgb = XGBClassifier()\n",
    "search = GridSearchCV(estimator = xgb, param_grid=grid)\n",
    "\n",
    "search.fit(X_train,y_train)\n",
    "    \n",
    "optimal_max_depth = search.best_params_['max_depth']\n",
    "optimal_learning_rate = search.best_params_['learning_rate']\n",
    "print('best max depth:',optimal_max_depth)\n",
    "print('best learning rate:',optimal_learning_rate)\n",
    "\n",
    "xgb = XGBClassifier(max_depth = optimal_max_depth, learning_rate = optimal_learning_rate)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "xgb.score(X_test,y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tuned the xgb model by creating a grid search that goes through different \n",
    "\n",
    "values for max_depth and learning_rate and finds the best value for it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
